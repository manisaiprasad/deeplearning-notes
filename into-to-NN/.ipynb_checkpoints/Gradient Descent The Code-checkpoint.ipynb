{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent: The Code\n",
    "From before we saw that one weight update can be calculated as:\n",
    "\n",
    "Δw \n",
    "i\n",
    "​\t =ηδx \n",
    "i\n",
    "\n",
    "with the error term \\deltaδ as\n",
    "\n",
    "δ=(y− \n",
    "y\n",
    "^\n",
    "​\t )f \n",
    "′\n",
    " (h)=(y− \n",
    "y\n",
    "^\n",
    "​\t )f \n",
    "′\n",
    " (∑w \n",
    "i\n",
    "​\t x \n",
    "i\n",
    "​\t )\n",
    "\n",
    "Remember, in the above equation (y− \n",
    "y\n",
    "^\n",
    "​\t ) is the output error, and f'(h)f \n",
    "′\n",
    " (h) refers to the derivative of the activation function, f(h)f(h). We'll call that derivative the output gradient.\n",
    "\n",
    "Now I'll write this out in code for the case of only one output unit. We'll also be using the sigmoid as the activation function  f(h)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Defining the sigmoid function for activations\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Input data\n",
    "x = np.array([0.1, 0.3])\n",
    "# Target\n",
    "y = 0.2\n",
    "# Input to output weights\n",
    "weights = np.array([-0.8, 0.5])\n",
    "\n",
    "# The learning rate, eta in the weight step equation\n",
    "learnrate = 0.5\n",
    "\n",
    "# the linear combination performed by the node (h in f(h) and f'(h))\n",
    "h = x[0]*weights[0] + x[1]*weights[1]\n",
    "# or h = np.dot(x, weights)\n",
    "\n",
    "# The neural network output (y-hat)\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# output error (y - y-hat)\n",
    "error = y - nn_output\n",
    "\n",
    "# output gradient (f'(h))\n",
    "output_grad = sigmoid_prime(h)\n",
    "\n",
    "# error term (lowercase delta)\n",
    "error_term = error * output_grad\n",
    "\n",
    "# Gradient descent step \n",
    "del_w = [ learnrate * error_term * x[0],\n",
    "          learnrate * error_term * x[1]]\n",
    "# or del_w = learnrate * error_term * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you are wondering where the derivative of the sigmoid function comes from (sigmoid_prime above), check out the derivation in this post.\n",
    "https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
